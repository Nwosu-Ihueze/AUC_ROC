{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AUC_ROCS.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyO2BoRa1uVnSNLNvTdZAplw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nwosu-Ihueze/AUC_ROC/blob/main/AUC_ROCS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8C32umDoQ5MX"
      },
      "source": [
        "# Import necessary packages\n",
        "import keras\n",
        "from keras import backend as K\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import cv2\n",
        "import sklearn\n",
        "import shap\n",
        "import os\n",
        "import seaborn as sns\n",
        "import time\n",
        "import pickle\n",
        " \n",
        "sns.set()\n",
        " \n",
        "# This sets a common size for all the figures we will draw.\n",
        "plt.rcParams['figure.figsize'] = [10, 7]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FoNu-OH4Q_1a"
      },
      "source": [
        "# Read csv file containing training datadata\n",
        "train_df = pd.read_csv(\"nih_new/train-small.csv\")\n",
        "valid_df = pd.read_csv(\"nih_new/valid-small.csv\")\n",
        "test_df = pd.read_csv(\"nih_new/test.csv\")\n",
        "print(f'There are {train_df.shape[0]} rows and {train_df.shape[1]} columns in the train data frame')\n",
        "train_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "arnHm5zcRDtG"
      },
      "source": [
        "print(f\"Train set: The total patient ids are {train_df['PatientId'].count()}, from those the unique ids are {train_df['PatientId'].value_counts().shape[0]} \")\n",
        "print(f\"Validation set: The total patient ids are {valid_df['PatientId'].count()}\")\n",
        "print(f\"Test set: The total patient ids are {test_df['PatientId'].count()}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Iv-ekNiRGb-"
      },
      "source": [
        "def check_for_leakage(df1, df2, patient_col):\n",
        "   \n",
        "    \n",
        "    df1_patients_unique = set(df1[patient_col])\n",
        "    df2_patients_unique = set(df2[patient_col])\n",
        "    \n",
        "    patients_in_both_groups = list(df1_patients_unique.intersection(df2_patients_unique))\n",
        " \n",
        "    # leakage contains true if there is patient overlap, otherwise false.\n",
        "    leakage = len(patients_in_both_groups) > 0 \n",
        "\n",
        "    \n",
        "    return leakage"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SQwu6cS5RQB3"
      },
      "source": [
        "print(\"leakage between train and test: {}\".format(check_for_leakage(train_df, test_df, 'PatientId')))\n",
        "print(\"leakage between valid and test: {}\".format(check_for_leakage(valid_df, test_df, 'PatientId')))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xv3COzCLRSwH"
      },
      "source": [
        "columns = train_df.keys()\n",
        "columns = list(columns)\n",
        "print(columns)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KHc1BwLORTp_"
      },
      "source": [
        "#Remove unnecesary elements\n",
        "columns.remove('Image')\n",
        "columns.remove('PatientId')\n",
        "# Get the total classes\n",
        "print(f\"There are {len(columns)} columns of labels for these conditions: {columns}\")\n",
        "# Print out the number of positive labels for each class\n",
        "for column in columns:\n",
        "    print(f\"The class {column} has {train_df[column].sum()} samples\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gEkc5i4URWeW"
      },
      "source": [
        "# Extract numpy values from Image column in data frame\n",
        "images = train_df['Image'].values\n",
        " \n",
        "# Extract 9 random images from it\n",
        "random_images = [np.random.choice(images) for i in range(9)]\n",
        " \n",
        "# Location of the image dir\n",
        "img_dir = 'nih_new/images-small/'\n",
        " \n",
        "print('Display Random Images')\n",
        " \n",
        "# Adjust the size of your images\n",
        "plt.figure(figsize=(20,10))\n",
        " \n",
        "# Iterate and plot random images\n",
        "for i in range(9):\n",
        "    plt.subplot(3, 3, i + 1)\n",
        "    img = plt.imread(os.path.join(img_dir, random_images[i]))\n",
        "    plt.imshow(img, cmap='gray')\n",
        "    plt.axis('off')\n",
        "    \n",
        "# Adjust subplot parameters to give specified padding\n",
        "plt.tight_layout()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EGMYUdtlRZfn"
      },
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ebp02tRaRciQ"
      },
      "source": [
        "def get_train_generator(df, image_dir, x_col, y_cols, shuffle=True, batch_size=8, seed=1, target_w = 320, target_h = 320):\n",
        "\n",
        "    \n",
        "    \n",
        "    print(\"getting train generator...\") \n",
        "    # Normalize images  --- Generate batches of tensor image data with real-time data augmentation\n",
        "    image_generator = ImageDataGenerator(\n",
        "        samplewise_center=True,              #Set each sample mean to 0\n",
        "        samplewise_std_normalization= True)  # Divide each input by its standard deviation\n",
        "\n",
        "    generator = image_generator.flow_from_dataframe(\n",
        "            dataframe=df,\n",
        "            directory=image_dir,\n",
        "            x_col=x_col,\n",
        "            y_col=y_cols,\n",
        "            class_mode=\"raw\",       #  Mode for yielding the targets, one of \"binary\", \"categorical\", \"input\", \"multi_output\", \"raw\", sparse\" or None. Default: \"categorical\".\n",
        "            batch_size=batch_size,\n",
        "            shuffle=shuffle,\n",
        "            seed=seed,\n",
        "            target_size=(target_w,target_h))\n",
        "    \n",
        "    return generator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q2rgke92RjeW"
      },
      "source": [
        "def get_test_and_valid_generator(valid_df, test_df, train_df, image_dir, x_col, y_cols, sample_size=100, batch_size=8, \n",
        "                                 seed=1, target_w = 320, target_h = 320):\n",
        "  \n",
        "    # get generator to sample dataset\n",
        "    print(f\"\\nextracting {sample_size} train images to normalize validation and test datasets...\")\n",
        " \n",
        "    raw_train_generator = ImageDataGenerator().flow_from_dataframe(\n",
        "        dataframe=train_df, \n",
        "        directory=IMAGE_DIR, \n",
        "        x_col=\"Image\", \n",
        "        y_col=labels, \n",
        "        class_mode=\"raw\", \n",
        "        batch_size=sample_size, \n",
        "        shuffle=True, \n",
        "        target_size=(target_w, target_h))\n",
        "    \n",
        "    # get data sample\n",
        "    batch = raw_train_generator.next() # generate a batch of samples and associated labels \n",
        "    data_sample = batch[0]             # => we need only the sample imgs ie batch[0]\n",
        " \n",
        "    # use sample to fit mean and std for test set generator\n",
        "    image_generator = ImageDataGenerator(\n",
        "        featurewise_center=True,\n",
        "        featurewise_std_normalization= True)\n",
        "    \n",
        "    # fit generator to sample from training data - we use this generator normalizing mean and std using the train sample of 100\n",
        "    image_generator.fit(data_sample)\n",
        "    \n",
        "    print(\"\\ngetting valid generator...\")\n",
        " \n",
        "    # get test generator\n",
        "    valid_generator = image_generator.flow_from_dataframe(\n",
        "            dataframe=valid_df,\n",
        "            directory=image_dir,\n",
        "            x_col=x_col,\n",
        "            y_col=y_cols,\n",
        "            class_mode=\"raw\",\n",
        "            batch_size=batch_size,\n",
        "            shuffle=False,\n",
        "            seed=seed,\n",
        "            target_size=(target_w,target_h))\n",
        "    \n",
        "    print(\"\\ngetting test generator...\")\n",
        "    test_generator = image_generator.flow_from_dataframe(\n",
        "            dataframe=test_df,\n",
        "            directory=image_dir,\n",
        "            x_col=x_col,\n",
        "            y_col=y_cols,\n",
        "            class_mode=\"raw\",\n",
        "            batch_size=batch_size,\n",
        "            shuffle=False,\n",
        "            seed=seed,\n",
        "            target_size=(target_w,target_h))\n",
        "    return valid_generator, test_generator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rg5ylBlERpa3"
      },
      "source": [
        "labels = ['Cardiomegaly', 'Emphysema', 'Effusion', 'Hernia', 'Infiltration', 'Mass', 'Nodule', 'Atelectasis',\n",
        "              'Pneumothorax', 'Pleural_Thickening', 'Pneumonia', 'Fibrosis', 'Edema', 'Consolidation']\n",
        "IMAGE_DIR = \"nih_new/images-small/\"\n",
        "train_generator = get_train_generator(train_df, IMAGE_DIR, \"Image\", labels)\n",
        "valid_generator, test_generator= get_test_and_valid_generator(valid_df, test_df, train_df, IMAGE_DIR, \"Image\", labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fVTQnD4CRs6X"
      },
      "source": [
        "def compute_class_freqs(labels):\n",
        "   \n",
        "    \n",
        "    # total number of patients (rows)\n",
        "    N = labels.shape[0]\n",
        "    \n",
        "    positive_frequencies = np.sum(labels, axis=0)/N\n",
        "    negative_frequencies = (N - np.sum(labels, axis=0))/N  # broadcasting of N to a line vector of dim num_classes\n",
        " \n",
        "   \n",
        "    return positive_frequencies, negative_frequencies"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OtUbHu8ORxKf"
      },
      "source": [
        "freq_pos, freq_neg = compute_class_freqs(train_generator.labels)\n",
        "freq_pos"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kiFHkEyUR41w"
      },
      "source": [
        "data = pd.DataFrame({\"Class\": labels, \"Label\": \"Positive\", \"Value\": freq_pos})\n",
        "data = data.append([{\"Class\": labels[l], \"Label\": \"Negative\", \"Value\": v} for l,v in enumerate(freq_neg)], ignore_index=True)\n",
        "plt.xticks(rotation=90)\n",
        "f = sns.barplot(x=\"Class\", y=\"Value\", hue=\"Label\" ,data=data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6DfeVAUWSAai"
      },
      "source": [
        "pos_weights = freq_neg\n",
        "neg_weights = freq_pos\n",
        "pos_contribution = freq_pos * pos_weights \n",
        "neg_contribution = freq_neg * neg_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hEX-KnwdSBhS"
      },
      "source": [
        "data = pd.DataFrame({\"Class\": labels, \"Label\": \"Positive\", \"Value\": pos_contribution})\n",
        "data = data.append([{\"Class\": labels[l], \"Label\": \"Negative\", \"Value\": v} \n",
        "                        for l,v in enumerate(neg_contribution)], ignore_index=True)\n",
        "plt.xticks(rotation=90)\n",
        "sns.barplot(x=\"Class\", y=\"Value\", hue=\"Label\" ,data=data);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zDi0apAZSFAx"
      },
      "source": [
        "def get_weighted_loss(pos_weights, neg_weights, epsilon=1e-7):\n",
        "  \n",
        "    def weighted_loss(y_true, y_pred):\n",
        "\n",
        "        # initialize loss to zero\n",
        "        loss = 0.0\n",
        "        \n",
        " \n",
        "        for i in range(len(pos_weights)):\n",
        "            # for each class, add average weighted loss for that class \n",
        "            loss += - pos_weights[i] * K.mean(y_true[:,i] * K.log(y_pred[:,i] + epsilon)) \\\n",
        "            - neg_weights[i] * K.mean((1-y_true[:,i]) * K.log(1-y_pred[:,i] + epsilon)) #complete this line\n",
        "        return loss\n",
        "    \n",
        "       \n",
        "    return weighted_loss             # this is a function taking 2 arguments y_true and y_pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I4droXvRSSzA"
      },
      "source": [
        "import keras\n",
        "from keras.applications.densenet import DenseNet121\n",
        "from keras.models import Model\n",
        "from keras.layers import Dense, Activation, Flatten, Dropout, BatchNormalization, GlobalAveragePooling2D\n",
        "from keras.callbacks import ModelCheckpoint, CSVLogger, LearningRateScheduler, ReduceLROnPlateau, EarlyStopping, TensorBoard\n",
        "from keras.preprocessing import image\n",
        " \n",
        "def load_C3M3_model():\n",
        "   \n",
        "    class_pos = train_df.loc[:, labels].sum(axis=0)\n",
        "    class_neg = len(train_df) - class_pos\n",
        "    class_total = class_pos + class_neg\n",
        " \n",
        "    pos_weights = class_pos / class_total\n",
        "    neg_weights = class_neg / class_total\n",
        "    print(\"Got loss weights\")\n",
        "    # create the base pre-trained model\n",
        "    base_model = DenseNet121(weights='densenet.hdf5', include_top=False)\n",
        "    print(\"Loaded DenseNet\")\n",
        "    # add a global spatial average pooling layer\n",
        "    x = base_model.output\n",
        "    x = GlobalAveragePooling2D()(x)\n",
        "    # and a logistic layer\n",
        "    predictions = Dense(len(labels), activation=\"sigmoid\")(x)\n",
        "    print(\"Added layers\")\n",
        " \n",
        "    model = Model(inputs=base_model.input, outputs=predictions)\n",
        "    \n",
        "    model.compile(optimizer='adam', loss=get_weighted_loss(neg_weights, pos_weights))\n",
        "    print(\"Compiled Model\")\n",
        " \n",
        "    model.load_weights(\"nih_new/pretrained_model.h5\")\n",
        "    print(\"Loaded Weights\")\n",
        " \n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x2JV2Z4VST0S"
      },
      "source": [
        "model = load_C3M3_model()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yiQLILFhSWv4"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NNAwtjkiSYvf"
      },
      "source": [
        "predicted_vals = model.predict(test_generator, steps = len(test_generator))\n",
        "predicted_vals.shape  # number of test samples x number of classes to predict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ylMVgTuScOh"
      },
      "source": [
        "from sklearn.metrics import roc_curve\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "def get_roc_curve(labels, predicted_vals, generator):\n",
        "    auc_roc_vals = []\n",
        "    for i in range(len(labels)):\n",
        "        try:\n",
        "            gt = generator.labels[:, i]\n",
        "            pred = predicted_vals[:, i]\n",
        "            auc_roc = roc_auc_score(gt, pred)\n",
        "            auc_roc_vals.append(auc_roc)\n",
        "            fpr_rf, tpr_rf, _ = roc_curve(gt, pred)\n",
        "            plt.figure(1, figsize=(10, 10))\n",
        "            plt.plot([0, 1], [0, 1], 'k--')\n",
        "            plt.plot(fpr_rf, tpr_rf,\n",
        "                     label=labels[i] + \" (\" + str(round(auc_roc, 3)) + \")\")\n",
        "            plt.xlabel('False positive rate')\n",
        "            plt.ylabel('True positive rate')\n",
        "            plt.title('ROC curve')\n",
        "            plt.legend(loc='best')\n",
        "        except:\n",
        "            print(\n",
        "                f\"Error in generating ROC curve for {labels[i]}. \"\n",
        "                f\"Dataset lacks enough examples.\"\n",
        "            )\n",
        "    plt.savefig('ROC.png')\n",
        "    plt.show()\n",
        "    return auc_roc_vals"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-5NOLwvkSfED"
      },
      "source": [
        "auc_rocs = get_roc_curve(labels, predicted_vals, test_generator)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}